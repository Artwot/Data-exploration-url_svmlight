{
 "cells": [
  {
   "source": [
    "# Clasificador K-NN en Spark usando pyspark.dataframes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Se importan las librerías necesarias"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import numpy as np "
   ]
  },
  {
   "source": [
    "### Se crea la sesión y config. de Spark"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[6]\") \\\n",
    "    .appName(\"Data exploration URL - KNN\") \\\n",
    "    .config(\"spark.executor.memory\", \"4gb\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('spark.executor.memory', '4gb'),\n",
       " ('spark.app.name', 'Data exploration URL - KNN'),\n",
       " ('spark.app.id', 'local-1617063923033'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.startTime', '1617063922367'),\n",
       " ('spark.master', 'local[6]'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/home/jsarabia/Documents/IA/Data-exploration-url_svmlight/code/spark-warehouse'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '33385'),\n",
       " ('spark.driver.host', 'fedora')]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<SparkContext master=local[6] appName=Data exploration URL - KNN>"
      ],
      "text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://fedora:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[6]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Data exploration URL - KNN</code></dd>\n            </dl>\n        </div>\n        "
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "source": [
    "### Se cargan los datos al dataframe "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"../data/url_svmlight/Dimension_100_x_500000.svm\")\n",
    "# Split the data into train and test\n",
    "seed = random.randrange(500, 1300, 2)\n",
    "splits = data.randomSplit([0.7, 0.3], 1234)\n",
    "\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "source": [
    "### Calcular la distancia euclideana.\n",
    "#### Summary:\n",
    "        Se calcula la distancia entre las columnas de dos renglones de un dataset, funciona\n",
    "        con argumentos provenientes de un renglón de un dataframe de Spark.\n",
    "#### Args: \n",
    "        row1(pyspark.sql.types.Row): Recibe una instancia del dataset\n",
    "        row2(pyspark.sql.types.Row): Recibe una instancia del dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    for column in range(len(row1[1])):\n",
    "        distance += pow(row1[1][column] - row2[1][column], 2)\n",
    "    distance = math.sqrt(distance)\n",
    "    return distance"
   ]
  },
  {
   "source": [
    "### Obtener los vecinos más cercanos.\n",
    "#### Summary: \n",
    "      Se recorre cada renglón del dataframe dado y se calcula la distancia entre cada \n",
    "      uno de estos y el renglón de prueba.\n",
    "      Se crea la lista \"distances\", la cual almacenar[a las distancias calculadas, \n",
    "      posteriormente se ordena de modo ascendente y se almancenan los primeros k-elementos \n",
    "      en la lista \"neighbors\"\n",
    "\n",
    "#### Args: \n",
    "      train(pyspark.sql.dataframe.DataFrame): Recibe el dataframe de entrenamiento\n",
    "      test_row(pyspark.sql.types.Row): Recibe una instancia o renglón del dataset\n",
    "      k(int): Número de k-vecinos que se desean obtener"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(train, test_row, k):\n",
    "    distances = []\n",
    "    total_train_rows = train.count() + 1\n",
    "    for train_row in range(1, total_train_rows):\n",
    "        distance = euclidean_distance(test_row, train.head(train_row)[-1])\n",
    "        if(distance != 0.0):\n",
    "           distances.append((train.head(train_row)[-1], distance))\n",
    "    distances.sort(key = lambda tup: tup[1])\n",
    "    neighbors = []\n",
    "    for i in range(k):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors"
   ]
  },
  {
   "source": [
    "### Predecir las etiquetas usando k-nn.\n",
    "#### Summary:\n",
    "      Se obtiene la lista de los k-vecinos más cercanos, y se almacena el valor de\n",
    "      la etiqueta en la lista \"output_labels\". Posteriormente se calcula el valor \n",
    "      promedio de las etiquetas y se almacena en la variable \"prediction\" y se retorna.\n",
    "\n",
    "#### Args: \n",
    "      train(pyspark.sql.dataframe.DataFrame): Recibe el dataframe de entrenamiento\n",
    "      test_row(pyspark.sql.types.Row): Recibe una instancia o renglón del dataset\n",
    "      k(int): Número de k-vecinos que se desean obtener"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classification(train, test_row, k):\n",
    "    neighbors = get_neighbors(train, test_row, k)\n",
    "    output_labels = [row[0] for row in neighbors]\n",
    "    prediction = max(set(output_labels), key=output_labels.count)\n",
    "    return prediction"
   ]
  },
  {
   "source": [
    "### Clacular el porcentaje de exactitud.\n",
    "#### Summary:\n",
    "      Esta función calcula el porcentaje de exactitud del uso de k-NN, comparando\n",
    "      las etiquetas reales de las instancias del dataset de entrenamiento y las\n",
    "      etiquetas obtenidas mediante la predicción usando k-NN.\n",
    "#### Args: \n",
    "      real_labels(pyspark.sql.dataframe.DataFrame): Recibe el dataframe de test que contiene los\n",
    "                                                    valores reales de las etiquetas\n",
    "      predicted(list): Lista con las etiquetas obtenidas mediante K-NN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(real_labels, predicted):\n",
    "    correct = 0\n",
    "    total_rows = real_labels.count() + 1\n",
    "    for i in range(1, total_rows):\n",
    "        if(real_labels.head(i)[0][0] == predicted[i - 1]):\n",
    "            correct += 1\n",
    "    print(\"Correct labels: \", correct, 'of', (total_rows - 1))\n",
    "    accuracy = correct / float(total_rows - 1)\n",
    "    return accuracy"
   ]
  },
  {
   "source": [
    "### Crear la función que calcule los vecinos más cercanos.\n",
    "#### Summary:\n",
    "      Se asignan los parámetros para calcular los k-vecinos más cercanos y hacer predicciones\n",
    "      de las etiquetas a las que pertenecen, calculando la distancia entre las columnas de cada\n",
    "      uno de los renglones del dataframe de \"test\" y el de \"train\", comparando las \n",
    "      reales con las otenidas por el clasificador y, finalmente, dado el porcentaje de exactitud obtenido. \n",
    "#### Args: \n",
    "      train(pyspark.sql.dataframe.DataFrame): Recibe el dataframe de entrenamiento\n",
    "      test_row(pyspark.sql.dataframe.DataFrame): Recibe el dataframe de test\n",
    "      k(int): Número de k-vecinos que se desean obtener"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(train, test, k):\n",
    "    predictions = []\n",
    "    total_test_rows = test.count() + 1\n",
    "    for test_row in range(1, total_test_rows):\n",
    "        output = predict_classification(train, test.head(test_row)[-1], k)\n",
    "        predictions.append(output)\n",
    "    mean_accuracy = accuracy(test, predictions)\n",
    "    print(\"Mean accuracy: \" + str(mean_accuracy))"
   ]
  },
  {
   "source": [
    "### Se llama al método y se envían los parámetros"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Correct labels:  32 of 69\nMean accuracy: 0.463768115942029\n"
     ]
    }
   ],
   "source": [
    "k_nearest_neighbors(train, test, k = 3)"
   ]
  },
  {
   "source": [
    "## Prueba de cada método de K-NN  con el archivo Dimensión 5 x 76"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dinstancia del renglon 1 con el 1 : 0.0\n",
      "Dinstancia del renglon 1 con el 2 : 3.1617478529944507\n",
      "Dinstancia del renglon 1 con el 3 : 2.0356611659021397\n",
      "Dinstancia del renglon 1 con el 4 : 1.4404646590152195\n",
      "Dinstancia del renglon 1 con el 5 : 1.7914667218708056\n",
      "Dinstancia del renglon 1 con el 6 : 2.8395962239571246\n",
      "Dinstancia del renglon 1 con el 7 : 2.4695692287281386\n",
      "Dinstancia del renglon 1 con el 8 : 3.161747853004572\n",
      "Dinstancia del renglon 1 con el 9 : 3.3159346823953433\n",
      "Dinstancia del renglon 1 con el 10 : 4.0780181152332435\n"
     ]
    }
   ],
   "source": [
    "# Prueba de la función euclidean_distance(), se mandan dos renglones del dataset total\n",
    "length = data.count()               # Se obtiene el total de renglones en el dataset\n",
    "for row in range(1, (length + 1)):\n",
    "    distance = euclidean_distance(data.head(1)[-1], data.head(row)[-1])\n",
    "    print(\"Dinstancia del renglon 1 con el\", row, \":\", distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Row(label=0.0, features=SparseVector(80, {3: 0.0539, 4: 0.0828, 5: 0.1176, 10: 0.2857, 15: 0.2, 16: 0.6557, 17: 0.7074, 18: 0.2835, 20: 0.2857, 21: 0.006, 23: 1.0, 27: 1.0, 35: 1.0, 43: 1.0, 53: 1.0, 55: 1.0, 61: 1.0, 63: 1.0, 65: 1.0, 67: 1.0, 69: 1.0, 71: 1.0, 73: 1.0, 75: 1.0}))\nRow(label=0.0, features=SparseVector(80, {3: 0.0539, 4: 0.0828, 5: 0.1176, 10: 0.2857, 15: 0.1, 16: 0.6168, 17: 0.8055, 18: 0.3137, 20: 0.1429, 21: 0.1429, 23: 1.0, 24: 1.0, 27: 1.0, 32: 0.0556, 40: 0.1, 53: 1.0, 55: 1.0, 61: 1.0, 63: 1.0, 65: 1.0, 67: 1.0, 69: 1.0, 71: 1.0, 73: 1.0, 75: 1.0}))\nRow(label=0.0, features=SparseVector(80, {1: 1.0, 3: 0.0622, 4: 0.0966, 5: 0.1176, 10: 0.2857, 16: 0.6339, 17: 0.788, 18: 0.2897, 20: 0.2857, 21: 0.0119, 23: 1.0, 24: 1.0, 27: 1.0, 32: 0.0556, 40: 0.1, 53: 1.0, 55: 1.0, 61: 1.0, 63: 1.0, 65: 1.0, 67: 1.0, 69: 1.0, 71: 1.0, 73: 1.0, 75: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "# Prueba de la función get_neighbors(), se envían como args. el dataframe, un renglón de este y el número de vecinos.\n",
    "length = data.count() + 1\n",
    "neighbors = get_neighbors(data, data.head(1)[-1], k=3)\n",
    "for neighbor in neighbors:\n",
    "    print(neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Row class: 0.0\n",
      "Expected label: 0, Got: 0.\n"
     ]
    }
   ],
   "source": [
    "# Prueba de la función get_neighbors(), se envían como args. el dataframe, un renglón de este y el número de vecinos y n es igual al número de renglón.\n",
    "n = 1\n",
    "print(\"Row class:\", data.head(n)[-1][0] ) # CLase/Label\n",
    "prediction = predict_classification(data, data.head(n)[-1], k=3)\n",
    "print('Expected label: %d, Got: %d.' % (data.head(n)[-1][0], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmento de pruebas locas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se asignan los RDD para el posterior procesamiento\n",
    "rdd_train = train.rdd\n",
    "rdd_test = test.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se utilizan dos array de numpy para alamacenar las instancias del set de entrenamiento y procesar con un RDD y el segundo array almacena las etiquetas. \n",
    "#train_array = np.array(train.select('features').collect(), dtype=float)\n",
    "#train_array_labels = np.array(train.select('label').collect(), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiquteas de las instancias del conjunto de test. \n",
    "test_array_labels = np.array(test.select('label').collect(), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RDD de entrenamiento: ' + str(rdd_train.count()))\n",
    "print('RDD de test: ' + str(rdd_test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo que guarda cada renglon en un archivo .svm\n",
    "def save_file(data):\n",
    "    file = open('../data/url_svmlight/Distancia_euclideana_100_x_500000.svm', 'a')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[summary:\n",
    "    Método para calcular la distancia euclídea entre cada una de las \n",
    "    columnas del conjunto de test respecto a las columnas del conjunto\n",
    "    de entrenamiento.\n",
    "]\n",
    "\n",
    "Args:\n",
    "    instance ([pyspark.sql.types.Row]): [\n",
    "        Recibe cada una de las instancias que hay en el dataset\n",
    "    ]\n",
    "\"\"\"\n",
    "def euclidean_distance(instance):\n",
    "    distance = 0\n",
    "    instance_distance = ''\n",
    "    for row in range(len(train_array)):\n",
    "        instance_distance += str(train_array_labels[row][0]) + ' '\n",
    "        for column in range(len(instance[1])):\n",
    "            distance = pow(train_array[row][0][column] - instance.features[column], 2)\n",
    "            distance = math.sqrt(distance)\n",
    "            # instance_distance += str(column + 1) +':' + str(distance) + ' ' # -> Si quisiera poner los indices de cada caracteristica.\n",
    "            instance_distance += str(distance) + ' '\n",
    "        instance_distance += '\\n'\n",
    "    save_file(instance_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecuta el método que calcula la distancia euclídea entre los puntos euclidean_distance()\n",
    "test.foreach(euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_samp1 = sc.textFile('../data/url_svmlight/arch_prb.svm')\n",
    "rdd_samp2 = sc.textFile('../data/url_svmlight/arch_prb1.svm')\n",
    "rdd_samp3 = sc.textFile('../data/url_svmlight/arch_prb2.svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_nearest1 = rdd_samp1.takeOrdered(5)\n",
    "five_nearest2 = rdd_samp2.takeOrdered(5)\n",
    "five_nearest3 = rdd_samp3.takeOrdered(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_average(five_nearest):\n",
    "    mean = 0\n",
    "    for i in range(5):\n",
    "        mean += float(five_nearest[i][0])\n",
    "    mean = mean / 5\n",
    "    if(mean > 0.5):\n",
    "        print('Clase K-NN: 1')\n",
    "        return 1\n",
    "    else:\n",
    "        print('Clase K-NN: 0')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy():\n",
    "    lista = [five_nearest1, five_nearest2, five_nearest3]\n",
    "    accuracy = 0.0\n",
    "    for i in range(len(test_array_labels)):\n",
    "        if(test_array_labels[i][0] == class_average(lista[i])):\n",
    "            accuracy += 1\n",
    "        print('Clase Real: ' + str(test_array_labels[i][0]))\n",
    "        print('\\n')\n",
    "    accuracy = accuracy / len(test_array_labels)\n",
    "    print('Accuracy: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = [five_nearest1, five_nearest2, five_nearest3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listaclass_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five_nearest[0][0] # Clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_prueba2 = sc.textFile('../data/url_svmlight/Distancia_euclideana_5_x_76.svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_nearest2 = rdd_prueba.takeOrdered(5)\n",
    "type(five_nearest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_nearest2[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python38564bitbaseconda596aabc4316c46b392e16f5639a7998d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}