{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-2-8796a381a593>:15 ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-8796a381a593>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;31m#sc = spark.sparkContext\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0msqlContext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSQLContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001B[0m\n\u001B[1;32m    142\u001B[0m                 \" is not allowed as it is a security risk.\")\n\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 144\u001B[0;31m         \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_ensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgateway\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    145\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36m_ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    340\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    341\u001B[0m                     \u001B[0;31m# Raise error if there is already a running Spark context\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 342\u001B[0;31m                     raise ValueError(\n\u001B[0m\u001B[1;32m    343\u001B[0m                         \u001B[0;34m\"Cannot run multiple SparkContexts at once; \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    344\u001B[0m                         \u001B[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-2-8796a381a593>:15 "
     ]
    }
   ],
   "source": [
    "# Build the SparkSession\n",
    "#spark = SparkSession.builder \\\n",
    "#    .master(\"local[6]\") \\\n",
    "#    .appName(\"Data exploration URL - KNN\") \\\n",
    "#    .config(\"spark.executor.memory\", \"4gb\") \\\n",
    "#    .getOrCreate()\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.executor.memory\", \"4gb\")\n",
    "conf.set(\"spark.driver.memory\",\"4g\")\n",
    "conf.set(\"spark.executor.cores\",\"6\")\n",
    "conf.set(\"spark.python.worker.memory\",\"4g\")\n",
    "\n",
    "#sc = spark.sparkContext\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[('spark.executor.memory', '4gb'),\n ('spark.app.id', 'local-1615751529673'),\n ('spark.driver.memory', '4g'),\n ('spark.executor.id', 'driver'),\n ('spark.driver.port', '34663'),\n ('spark.app.startTime', '1615751528882'),\n ('spark.app.name', 'pyspark-shell'),\n ('spark.executor.cores', '6'),\n ('spark.rdd.compress', 'True'),\n ('spark.python.worker.memory', '4g'),\n ('spark.serializer.objectStreamReset', '100'),\n ('spark.master', 'local[*]'),\n ('spark.submit.pyFiles', ''),\n ('spark.submit.deployMode', 'client'),\n ('spark.ui.showConsoleProgress', 'true'),\n ('spark.driver.host', 'fedora')]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = sqlContext.read.format(\"libsvm\")\\\n",
    "    .load(\"../data/url_svmlight/Dimension_100_x_100.svm\")\n",
    "# Split the data into train and test\n",
    "seed = random.randrange(500, 1300, 2)\n",
    "splits = data.randomSplit([0.7, 0.3], 1234)\n",
    "\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Metodo que guarda cada renglon en un archivo .svm\n",
    "def save_file(data):\n",
    "    file = open('../data/sklearn_format/url_100_x_100.csv', 'a')\n",
    "    file.write(data)\n",
    "    file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def change_format(instance):\n",
    "    concat_instance = ''\n",
    "    for column in range(len(instance[1])):\n",
    "        concat_instance += str(instance.features[column]) + ', '\n",
    "        # print(instance.features[column])\n",
    "    concat_instance += str(instance.label) + '\\n'\n",
    "    save_file(concat_instance)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "data.foreach(change_format)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/jsarabia/Documents/IA/Data-exploration-url_svmlight/code'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}