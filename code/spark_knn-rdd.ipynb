{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador K-NN en Spark usando pyspark.dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se importan las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se crea la sesión y config. de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "        .setAppName(\"Data exploration URL - KNN Spark RDD\") \\\n",
    "        .set('spark.driver.cores', '6') \\\n",
    "        .set('spark.executor.cores', '6') \\\n",
    "        .set('spark.driver.memory', '7G') \\\n",
    "        .set('spark.executor.memory', '7G'))\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.startTime', '1618773523232'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.port', '40455'),\n",
       " ('spark.driver.cores', '6'),\n",
       " ('spark.driver.memory', '7G'),\n",
       " ('spark.executor.cores', '6'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.memory', '7G'),\n",
       " ('spark.app.id', 'local-1618773525351'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'Data exploration URL - KNN Spark RDD'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', 'fedora')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fedora:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Data exploration URL - KNN Spark RDD</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Data exploration URL - KNN Spark RDD>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para calcular el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiempo(start, end):\n",
    "    medida = 'segundos'\n",
    "    tiempo = end - start\n",
    "    if (tiempo >= 60):\n",
    "        tiempo = tiempo / 60\n",
    "        medida = 'minutos'\n",
    "    print(\"Tiempo de ejecución: \", round(tiempo, 2), medida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular la distancia euclideana.\n",
    "#### Summary:\n",
    "        Se calcula la distancia entre las columnas de dos renglones de un dataset, funciona\n",
    "        con argumentos provenientes de un renglón de un dataframe de Spark.\n",
    "#### Args: \n",
    "        row1(numpy.ndarray): Recibe una instancia del dataset\n",
    "        row2(pyspark.ml.linalg.SparseVector): Recibe una instancia del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    columns = len(row1[0])\n",
    "    for column in range(columns):\n",
    "        distance += pow(row1[0][column] - row2[column], 2)\n",
    "    distance = math.sqrt(distance)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener los vecinos más cercanos.\n",
    "#### Summary: \n",
    "      Se recorre cada renglón del dataframe dado y se calcula la distancia entre cada \n",
    "      uno de estos y el renglón de prueba.\n",
    "      El RDD \"distances\", almacenará las distancias calculadas, \n",
    "      posteriormente se ordena de modo ascendente y se almancenan los primeros k-elementos \n",
    "      en la lista \"k_neighbors\"\n",
    "\n",
    "#### Args: \n",
    "      train(pyspark.rdd.RDD): Recibe el conjunto de entrenamiento\n",
    "      test_row(numpy.ndarray): Recibe una instancia del conjunto de test\n",
    "      k(int): Número de vecinos que se desean obtener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(train, test_row, k):\n",
    "    rdd_distances = train.map(lambda element: (element[0], euclidean_distance(test_row, element[1])))\n",
    "    rdd_distances = rdd_distances.filter(lambda element: element[1] > 0.0)\n",
    "    k_neighbors = rdd_distances.takeOrdered(k, key= lambda  x: x[1]) \n",
    "    return k_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predecir las etiquetas usando k-nn.\n",
    "#### Summary:\n",
    "      Se obtiene la lista de los k-vecinos más cercanos, y se almacena el valor de\n",
    "      la etiqueta en la lista \"output_labels\". Posteriormente se calcula el valor \n",
    "      promedio de las etiquetas y se almacena en la variable \"prediction\" y se retorna.\n",
    "\n",
    "#### Args: \n",
    "      train(pyspark.rdd.RDD): Recibe el conjunto de entrenamiento\n",
    "      test_row(numpy.ndarray): Recibe una instancia del conjunto de test\n",
    "      k(int): Número de vecinos que se desean obtener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classification(train, test_row, k):\n",
    "    neighbors = get_neighbors(train, test_row, k)\n",
    "    output_labels = [row[0] for row in neighbors]\n",
    "    prediction = max(set(output_labels), key=output_labels.count)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clacular el porcentaje de exactitud.\n",
    "#### Summary:\n",
    "      Esta función calcula el porcentaje de exactitud del uso de k-NN, comparando\n",
    "      las etiquetas reales de las instancias del dataset de entrenamiento y las\n",
    "      etiquetas obtenidas mediante la predicción usando k-NN.\n",
    "#### Args: \n",
    "      real_labels(numpy.ndarray): Recibe el dataframe de test que contiene los\n",
    "                                                    valores reales de las etiquetas\n",
    "      predicted(list): Lista con las etiquetas obtenidas mediante K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(real_labels, predicted):\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total_rows = real_labels.count()\n",
    "    print(total_rows)\n",
    "    for i in range(total_rows):\n",
    "        real_label = np.array(real_labels.zipWithIndex().filter(lambda element: element[1] == i).map(lambda element: element[0][0]).collect(), dtype = float)\n",
    "        if(real_label == predicted[i]):\n",
    "            correct += 1\n",
    "    print(\"Correct labels: \", correct, 'of', (total_rows))\n",
    "    accuracy = correct / float(total_rows)\n",
    "    return accuracy\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total_rows = len(real_labels)\n",
    "    for i in range(total_rows):\n",
    "        if(real_labels[i] == predicted[i]):\n",
    "            correct += 1\n",
    "    print(\"Correct labels: \", correct, 'of', (total_rows))\n",
    "    accuracy = correct / float(total_rows)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear la función que calcule los vecinos más cercanos.\n",
    "#### Summary:\n",
    "      Se asignan los parámetros para calcular los k-vecinos más cercanos y hacer predicciones\n",
    "      de las etiquetas a las que pertenecen, calculando la distancia entre las columnas de cada\n",
    "      uno de los renglones del dataframe de \"test\" y el de \"train\", comparando las \n",
    "      reales con las otenidas por el clasificador y, finalmente, dado el porcentaje de exactitud obtenido. \n",
    "#### Args: \n",
    "      train(pyspark.rdd.RDD): Recibe el conjunto de entrenamiento\n",
    "      test(pyspark.rdd.RDD): Recibe el conjunto de test\n",
    "      k(int): Número de vecinos que se desean obtener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(train, test, k):\n",
    "    predictions = []\n",
    "    total_test_rows = test.count()\n",
    "    for index in range(total_test_rows):\n",
    "        test_row = np.array(test.zipWithIndex().filter(lambda element: element[1] == index).map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "        output = predict_classification(train, test_row, k)\n",
    "        predictions.append(output)\n",
    "    labels_array = np.array(test.map(lambda x: x[0]).collect(), dtype = float)\n",
    "    mean_accuracy = accuracy(labels_array, predictions)\n",
    "    print(\"Mean accuracy: \" + str(mean_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se cargan los datos al dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .option(\"header\", \"false\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .load(\"../data/url_svmlight/Dimension_100_x_1000.svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"features_norm\")\n",
    "\n",
    "# Compute summary statistics and generate MaxAbsScalerModel\n",
    "scalerModel = scaler.fit(data)\n",
    "\n",
    "# rescale each feature to range [-1, 1].\n",
    "scaledData = scalerModel.transform(data)\n",
    "\n",
    "scaledData = scaledData.drop(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features_norm=SparseVector(987, {1: 1.0, 3: 0.2745, 4: 0.4667, 5: 0.1667, 9: 1.0, 10: 0.1429, 16: 0.9928, 17: 0.9858, 18: 0.2117, 20: 1.0, 21: 0.0139, 22: 0.0208, 23: 1.0, 32: 1.0, 36: 1.0, 40: 1.0, 44: 1.0, 47: 1.0, 53: 1.0, 55: 1.0, 61: 1.0, 63: 1.0, 65: 1.0, 67: 1.0, 69: 1.0, 71: 1.0, 73: 1.0, 75: 1.0, 78: 0.2, 80: 0.1667, 81: 1.0, 83: 1.0, 85: 1.0, 87: 1.0, 89: 1.0, 91: 1.0, 93: 1.0, 95: 1.0, 101: 1.0, 103: 1.0, 105: 1.0, 107: 1.0, 109: 1.0, 111: 1.0, 130: 1.0, 132: 1.0, 138: 1.0, 140: 1.0, 142: 1.0, 144: 1.0, 146: 1.0, 148: 1.0, 252: 1.0, 278: 1.0, 385: 1.0, 497: 1.0, 604: 1.0, 638: 1.0, 640: 1.0}))]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dividir los datos en conjunto de train y de test\n",
    "seed = 1234\n",
    "splits = scaledData.randomSplit([0.7, 0.3], seed)\n",
    "\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# Se asignan los RDD para el posterior procesamiento\n",
    "rdd_train = train.rdd\n",
    "rdd_test = test.rdd\n",
    "# rdd_total = data.rdd\n",
    "\n",
    "scaledData.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se invoca al método y se envían los parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct labels:  55 of 69\n",
      "Mean accuracy: 0.7971014492753623\n",
      "Tiempo de ejecución:  34.28 segundos\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "k_nearest_neighbors(rdd_train, rdd_test, k = 5)\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time, end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de cada método de K-NN  con el archivo Dimensión 5 x 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se asignan los RDD para el posterior procesamiento\n",
    "rdd_train = train.rdd\n",
    "rdd_test = test.rdd\n",
    "rdd_total = data.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se agrega un índice a las instancias para poder recorrerlas posteriormente mediante un filtro.\n",
    "rdd_index = rdd_total.zipWithIndex()\n",
    "# Se selecciona solo la columna que contiene los valores de las características.\n",
    "rdd_columns = rdd_total.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución:  0.29 segundos\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Se prueba el método de distancia euclideana con RDD\n",
    "# Renglón no. 1\n",
    "rdd_row1 = rdd_index.filter(lambda x: x[1] == 0)\n",
    "# Se transforma en un array de Numpy solo con los valores de las columnas\n",
    "row1 = np.array(rdd_row1.map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "# Las distancias se almacenan en un RDD\n",
    "rdd_distances = rdd_columns.map(lambda x: euclidean_distance(row1, x))\n",
    "start_time = time.time()\n",
    "rdd_distances.collect()\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time,end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 2.0243685305406998), (0.0, 2.02465364064394), (0.0, 2.031920353046332)]\n",
      "Tiempo de ejecución:  0.2687826156616211 segundos\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Prueba de la función get_neighbors()\n",
    "# Renglón no. 1\n",
    "rdd_row1 = rdd_index.filter(lambda x: x[1] == 0)\n",
    "# Se transforma en un array de Numpy solo con los valores de las columnas\n",
    "row1 = np.array(rdd_row1.map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "start_time = time.time()\n",
    "print(get_neighbors(rdd_total, row1, 3))\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time,end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected label: 0, Got: 0.\n",
      "Tiempo de ejecución:  0.6830856800079346\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Prueba de la función predict_classification()\n",
    "# Renglón no. 1\n",
    "rdd_row1 = rdd_index.filter(lambda x: x[1] == 0)\n",
    "# Se transforma en un array de Numpy solo con los valores de las columnas\n",
    "row1 = np.array(rdd_row1.map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "start_time = time.time()\n",
    "prediction = predict_classification(rdd_total, row1, 3)\n",
    "print('Expected label: %d, Got: %d.' % (rdd_row1.take(1)[0][0][0], prediction))\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time,end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd04b885024a669b25ee4a71b7d0638ae4cd28c0f16f4c7c66f708405d8a6800548"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "4b885024a669b25ee4a71b7d0638ae4cd28c0f16f4c7c66f708405d8a6800548"
   }
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
