{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador K-NN en Spark usando pyspark.RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se importan las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se crea la sesión y config. de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "        .setAppName(\"Data exploration URL - KNN Spark RDD\") \\\n",
    "        .set('spark.driver.cores', '6') \\\n",
    "        .set('spark.executor.cores', '6') \\\n",
    "        .set('spark.driver.memory', '6G') \\\n",
    "        .set('spark.master', 'local[6]') \\\n",
    "        .set('spark.sql.autoBroadcastJoinThreshold', '-1') \\\n",
    "        .set('spark.executor.memory', '6G'))\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '32967'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.memory', '6G'),\n",
       " ('spark.app.startTime', '1619200389577'),\n",
       " ('spark.app.id', 'local-1619200390025'),\n",
       " ('spark.master', 'local[6]'),\n",
       " ('spark.driver.cores', '6'),\n",
       " ('spark.executor.memory', '6G'),\n",
       " ('spark.executor.cores', '6'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.autoBroadcastJoinThreshold', '-1'),\n",
       " ('spark.app.name', 'Data exploration URL - KNN Spark RDD'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', 'fedora')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fedora:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[6]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Data exploration URL - KNN Spark RDD</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[6] appName=Data exploration URL - KNN Spark RDD>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para calcular el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiempo(start, end):\n",
    "    medida = 'segundos'\n",
    "    tiempo = end - start\n",
    "    if (tiempo >= 60):\n",
    "        tiempo = tiempo / 60\n",
    "        medida = 'minutos'\n",
    "    else:\n",
    "        if (tiempo >= 3600):\n",
    "            tiempo = tiempo / 3600\n",
    "            medida = 'horas'\n",
    "    print(\"Tiempo de ejecución: \", round(tiempo, 2), medida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular la distancia euclideana.\n",
    "#### Summary:\n",
    "        Se calcula la distancia entre las columnas de dos renglones de un dataset, funciona\n",
    "        con argumentos provenientes de un renglón de un dataframe de Spark.\n",
    "#### Args: \n",
    "        row1(numpy.ndarray): Recibe una instancia del dataset\n",
    "        row2(pyspark.ml.linalg.SparseVector): Recibe una instancia del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    columns = len(row1[0])\n",
    "    for column in range(columns):\n",
    "        distance += pow(row1[0][column] - row2[column], 2)\n",
    "    distance = math.sqrt(distance)\n",
    "    return round(distance, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener los vecinos más cercanos.\n",
    "#### Summary: \n",
    "      Se recorre cada renglón del dataframe dado y se calcula la distancia entre cada \n",
    "      uno de estos y el renglón de prueba.\n",
    "      El RDD \"distances\", almacenará las distancias calculadas, \n",
    "      posteriormente se ordena de modo ascendente y se almancenan los primeros k-elementos \n",
    "      en la lista \"k_neighbors\"\n",
    "\n",
    "#### Args: \n",
    "      train(pyspark.rdd.RDD): Recibe el conjunto de entrenamiento\n",
    "      test_row(numpy.ndarray): Recibe una instancia del conjunto de test\n",
    "      k(int): Número de vecinos que se desean obtener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(train, test_row, k):\n",
    "    rdd_distances = train.map(lambda element: (element[0], euclidean_distance(test_row, element[1])))\n",
    "    rdd_distances = rdd_distances.filter(lambda element: element[1] > 0.0)\n",
    "    rdd_distances.\n",
    "    k_neighbors = rdd_distances.takeOrdered(k, key= lambda  x: x[1]) \n",
    "    return k_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def sort_neighbors(rdd_distances, k):\n",
    "    lista = []\n",
    "    for i in range(k):\n",
    "        element = rdd_distances.min()\n",
    "        rddAux = rdd_distances.filter(lambda x: x != element)\n",
    "        lista.append(element)\n",
    "    return lista\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predecir las etiquetas usando k-nn.\n",
    "#### Summary:\n",
    "      Se obtiene la lista de los k-vecinos más cercanos, y se almacena el valor de\n",
    "      la etiqueta en la lista \"output_labels\". Posteriormente se calcula el valor \n",
    "      promedio de las etiquetas y se almacena en la variable \"prediction\" y se retorna.\n",
    "\n",
    "#### Args: \n",
    "      train(pyspark.rdd.RDD): Recibe el conjunto de entrenamiento\n",
    "      test_row(numpy.ndarray): Recibe una instancia del conjunto de test\n",
    "      k(int): Número de vecinos que se desean obtener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classification(train, test_row, k):\n",
    "    neighbors = get_neighbors(train, test_row, k)\n",
    "    output_labels = [row[0] for row in neighbors]\n",
    "    prediction = max(set(output_labels), key=output_labels.count)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clacular el porcentaje de exactitud.\n",
    "#### Summary:\n",
    "      Esta función calcula el porcentaje de exactitud del uso de k-NN, comparando\n",
    "      las etiquetas reales de las instancias del dataset de entrenamiento y las\n",
    "      etiquetas obtenidas mediante la predicción usando k-NN.\n",
    "#### Args: \n",
    "      real_labels(numpy.ndarray): Recibe el dataframe de test que contiene los\n",
    "                                                    valores reales de las etiquetas\n",
    "      predicted(list): Lista con las etiquetas obtenidas mediante K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(real_labels, predicted):\n",
    "    correct = 0\n",
    "    total_rows = len(real_labels)\n",
    "    for i in range(total_rows):\n",
    "        if(real_labels[i] == predicted[i]):\n",
    "            correct += 1\n",
    "    print(\"Correct labels: \", correct, 'of', (total_rows))\n",
    "    accuracy = correct / float(total_rows)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear la función que calcule los vecinos más cercanos.\n",
    "#### Summary:\n",
    "      Se asignan los parámetros para calcular los k-vecinos más cercanos y hacer predicciones\n",
    "      de las etiquetas a las que pertenecen, calculando la distancia entre las columnas de cada\n",
    "      uno de los renglones del dataframe de \"test\" y el de \"train\", comparando las \n",
    "      reales con las otenidas por el clasificador y, finalmente, dado el porcentaje de exactitud obtenido. \n",
    "#### Args: \n",
    "      train(pyspark.rdd.RDD): Recibe el conjunto de entrenamiento\n",
    "      test(pyspark.rdd.RDD): Recibe el conjunto de test\n",
    "      k(int): Número de vecinos que se desean obtener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(train, test, k):\n",
    "    predictions = []\n",
    "    total_test_rows = test.count()\n",
    "    for index in range(total_test_rows):\n",
    "        test_row = np.array(test.zipWithIndex().filter(lambda element: element[1] == index).map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "        output = predict_classification(train, test_row, k)\n",
    "        predictions.append(output)\n",
    "    labels_array = np.array(test.map(lambda x: x[0]).collect(), dtype = float)\n",
    "    mean_accuracy = accuracy(labels_array, predictions)\n",
    "    print(\"Mean accuracy: \" + str(mean_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se cargan los datos al dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .option(\"header\", \"false\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .load(\"/home/jsarabia/Documents/IA/datasets/Data-exploration/807990_x_instances_30.svm\")\n",
    "    # .load(\"../data/url_svmlight/Dimension_100_x_1000.svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"features_norm\")\n",
    "\n",
    "# Compute summary statistics and generate MaxAbsScalerModel\n",
    "scalerModel = scaler.fit(data)\n",
    "\n",
    "# rescale each feature to range [-1, 1].\n",
    "scaledData = scalerModel.transform(data)\n",
    "\n",
    "scaledData = scaledData.drop(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features_norm=SparseVector(763908, {1: 1.0, 3: 0.1923, 4: 0.3043, 5: 0.4, 10: 1.0, 16: 0.9776, 17: 0.9894, 18: 0.1567, 20: 0.0496, 21: 0.1488, 22: 0.1488, 23: 1.0, 24: 1.0, 35: 1.0, 36: 1.0, 43: 1.0, 44: 1.0, 47: 1.0, 49: 1.0, 53: 1.0, 55: 1.0, 63: 1.0, 69: 1.0, 71: 1.0, 73: 1.0, 75: 1.0, 83: 1.0, 89: 1.0, 91: 1.0, 93: 1.0, 95: 1.0, 103: 1.0, 109: 1.0, 111: 1.0, 130: 1.0, 132: 1.0, 140: 1.0, 146: 1.0, 148: 1.0, 287: 1.0, 331: 1.0, 332: 1.0, 333: 1.0, 338: 1.0, 339: 1.0, 340: 1.0, 359: 1.0, 360: 1.0, 385: 1.0, 387: 1.0, 389: 1.0, 593: 1.0, 604: 1.0, 695: 1.0, 729: 1.0, 763: 1.0, 764: 1.0, 765: 1.0, 841: 1.0, 1678: 1.0, 1739: 1.0, 1740: 1.0, 1741: 1.0, 4291: 1.0, 8340: 1.0, 34065: 1.0, 34066: 1.0, 34067: 1.0, 34068: 1.0, 47172: 1.0, 82032: 1.0, 82033: 1.0, 82034: 1.0, 82035: 1.0, 90693: 1.0, 90694: 1.0, 155152: 1.0, 155153: 1.0, 155154: 1.0, 155155: 1.0, 155156: 1.0, 155157: 1.0, 155158: 1.0, 155159: 1.0, 155162: 1.0, 155163: 1.0, 155164: 1.0, 155165: 1.0, 155166: 1.0, 155167: 1.0, 155168: 1.0, 155169: 1.0, 155170: 1.0, 155171: 1.0, 155172: 1.0, 155173: 1.0, 155174: 1.0, 155175: 1.0, 155176: 1.0, 155177: 1.0, 155178: 1.0, 155179: 1.0, 155180: 1.0, 155181: 1.0, 155182: 1.0, 155183: 1.0, 155184: 1.0, 155185: 1.0, 155186: 1.0, 155187: 1.0, 155188: 1.0, 155189: 1.0, 155190: 1.0, 155191: 1.0, 155192: 1.0, 155193: 1.0, 155194: 1.0, 155195: 1.0, 155196: 1.0, 155197: 1.0, 155198: 1.0, 155199: 1.0, 155200: 1.0, 155201: 1.0, 155202: 1.0, 155203: 1.0, 155204: 1.0, 155205: 1.0, 155206: 1.0, 155207: 1.0, 155208: 1.0, 155209: 1.0, 155210: 1.0, 155211: 1.0, 155212: 1.0, 379663: 1.0}))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dividir los datos en conjunto de train y de test\n",
    "seed = 1234\n",
    "splits = scaledData.randomSplit([0.7, 0.3], seed)\n",
    "\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# Se asignan los RDD para el posterior procesamiento\n",
    "rdd_train = train.rdd\n",
    "rdd_test = test.rdd\n",
    "# rdd_total = data.rdd\n",
    "\n",
    "scaledData.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se invoca al método y se envían los parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct labels:  14 of 18\n",
      "Mean accuracy: 0.7777777777777778\n",
      "Tiempo de ejecución:  44.45 minutos\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "k_nearest_neighbors(rdd_train, rdd_test, k = 5)\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time, end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark session stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de cada método de K-NN  con el archivo Dimensión 5 x 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se asignan los RDD para el posterior procesamiento\n",
    "rdd_train = train.rdd\n",
    "rdd_test = test.rdd\n",
    "rdd_total = data.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se agrega un índice a las instancias para poder recorrerlas posteriormente mediante un filtro.\n",
    "rdd_index = rdd_total.zipWithIndex()\n",
    "# Se selecciona solo la columna que contiene los valores de las características.\n",
    "rdd_columns = rdd_total.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución:  0.08 segundos\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Se prueba el método de distancia euclideana con RDD\n",
    "# Renglón no. 1\n",
    "rdd_row1 = rdd_index.filter(lambda x: x[1] == 0)\n",
    "# Se transforma en un array de Numpy solo con los valores de las columnas\n",
    "row1 = np.array(rdd_row1.map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "# Las distancias se almacenan en un RDD\n",
    "rdd_distances = rdd_columns.map(lambda x: euclidean_distance(row1, x))\n",
    "start_time = time.time()\n",
    "rdd_distances.collect()\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time,end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0\n",
      "Tiempo de ejecución:  0.69 segundos\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Prueba de la función get_neighbors()\n",
    "# Renglón no. 1\n",
    "rdd_row1 = rdd_index.filter(lambda x: x[1] == 0)\n",
    "# Se transforma en un array de Numpy solo con los valores de las columnas\n",
    "row1 = np.array(rdd_row1.map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "start_time = time.time()\n",
    "print(get_neighbors(rdd_total, row1, k = 5))\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time,end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected label: 0, Got: 0.\n",
      "Tiempo de ejecución:  0.6830856800079346\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Prueba de la función predict_classification()\n",
    "# Renglón no. 1\n",
    "rdd_row1 = rdd_index.filter(lambda x: x[1] == 0)\n",
    "# Se transforma en un array de Numpy solo con los valores de las columnas\n",
    "row1 = np.array(rdd_row1.map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "start_time = time.time()\n",
    "prediction = predict_classification(rdd_total, row1, 3)\n",
    "print('Expected label: %d, Got: %d.' % (rdd_row1.take(1)[0][0][0], prediction))\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time,end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(1,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.filter(lambda x: x != 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.filter(lambda x: x != 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 5, 6, 7, 8, 10]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.filter(lambda x: x != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 5, 6, 7, 8, 10]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_vecinos = sc.parallelize([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_vecinos_prb = rdd_vecinos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 2.2022),\n",
       " (1.0, 4.1898),\n",
       " (1.0, 3.8775),\n",
       " (1.0, 3.7552),\n",
       " (1.0, 3.7552),\n",
       " (1.0, 3.2155)]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_vecinos_prb.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "element = rdd_vecinos_prb.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 2.2022)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_vecinos_prb = rdd_vecinos_prb.filter(lambda x: x != element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 1.6999),\n",
       " (1.0, 4.1898),\n",
       " (1.0, 3.8775),\n",
       " (1.0, 3.7552),\n",
       " (1.0, 3.7552),\n",
       " (1.0, 3.2155)]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_vecinos_prb.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 619\n"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd04b885024a669b25ee4a71b7d0638ae4cd28c0f16f4c7c66f708405d8a6800548"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "4b885024a669b25ee4a71b7d0638ae4cd28c0f16f4c7c66f708405d8a6800548"
   }
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
