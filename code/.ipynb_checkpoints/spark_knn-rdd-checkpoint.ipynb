{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador K-NN en Spark usando pyspark.dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se importan las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se crea la sesión y config. de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "        .setAppName(\"Data exploration URL - KNN Spark RDD\") \\\n",
    "        .set('spark.driver.cores', '5') \\\n",
    "        .set('spark.executor.cores', '5') \\\n",
    "        .set('spark.driver.memory', '5G') \\\n",
    "        .set('spark.executor.memory', '5G'))\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '35969'),\n",
       " ('spark.executor.cores', '5'),\n",
       " ('spark.executor.memory', '5G'),\n",
       " ('spark.app.id', 'local-1618589946041'),\n",
       " ('spark.driver.cores', '5'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.memory', '5G'),\n",
       " ('spark.app.startTime', '1618589942839'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'Data exploration URL - KNN Spark RDD'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', 'fedora')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fedora:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Data exploration URL - KNN Spark RDD</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Data exploration URL - KNN Spark RDD>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def tiempo(start, end):\n",
    "    medida = ' segundos'\n",
    "    tiempo = end - start\n",
    "    if (tiempo >= 60):\n",
    "        tiempo = tiempo / 60\n",
    "        medida = ' minutos'\n",
    "    print(\"Tiempo de ejecución: \", tiempo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular la distancia euclideana.\n",
    "#### Summary:\n",
    "        Se calcula la distancia entre las columnas de dos renglones de un dataset, funciona\n",
    "        con argumentos provenientes de un renglón de un dataframe de Spark.\n",
    "#### Args: \n",
    "        row1(numpy.ndarray): Recibe una instancia del dataset\n",
    "        row2(pyspark.ml.linalg.SparseVector): Recibe una instancia del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    columns = len(row1[0])\n",
    "    for column in range(columns):\n",
    "        distance += pow(row1[0][column] - row2[column], 2)\n",
    "    distance = math.sqrt(distance)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener los vecinos más cercanos.\n",
    "#### Summary: \n",
    "      Se recorre cada renglón del dataframe dado y se calcula la distancia entre cada \n",
    "      uno de estos y el renglón de prueba.\n",
    "      El RDD \"distances\", almacenará las distancias calculadas, \n",
    "      posteriormente se ordena de modo ascendente y se almancenan los primeros k-elementos \n",
    "      en la lista \"k_neighbors\"\n",
    "\n",
    "#### Args: \n",
    "      train(pyspark.rdd.RDD): Recibe el conjunto de entrenamiento\n",
    "      test_row(numpy.ndarray): Recibe una instancia del conjunto de test\n",
    "      k(int): Número de vecinos que se desean obtener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(train, test_row, k):\n",
    "    rdd_distances = train.map(lambda element: (element[0], euclidean_distance(test_row, element[1])))\n",
    "    rdd_distances = rdd_distances.filter(lambda element: element[1] > 0.0)\n",
    "    k_neighbors = rdd_distances.takeOrdered(k, key= lambda  x: x[1])\n",
    "    return k_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predecir las etiquetas usando k-nn.\n",
    "#### Summary:\n",
    "      Se obtiene la lista de los k-vecinos más cercanos, y se almacena el valor de\n",
    "      la etiqueta en la lista \"output_labels\". Posteriormente se calcula el valor \n",
    "      promedio de las etiquetas y se almacena en la variable \"prediction\" y se retorna.\n",
    "\n",
    "#### Args: \n",
    "      train(pyspark.rdd.RDD): Recibe el conjunto de entrenamiento\n",
    "      test_row(numpy.ndarray): Recibe una instancia del conjunto de test\n",
    "      k(int): Número de vecinos que se desean obtener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classification(train, test_row, k):\n",
    "    neighbors = get_neighbors(train, test_row, k)\n",
    "    output_labels = [row[0] for row in neighbors]\n",
    "    prediction = max(set(output_labels), key=output_labels.count)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clacular el porcentaje de exactitud.\n",
    "#### Summary:\n",
    "      Esta función calcula el porcentaje de exactitud del uso de k-NN, comparando\n",
    "      las etiquetas reales de las instancias del dataset de entrenamiento y las\n",
    "      etiquetas obtenidas mediante la predicción usando k-NN.\n",
    "#### Args: \n",
    "      real_labels(numpy.ndarray): Recibe el dataframe de test que contiene los\n",
    "                                                    valores reales de las etiquetas\n",
    "      predicted(list): Lista con las etiquetas obtenidas mediante K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(real_labels, predicted):\n",
    "    correct = 0\n",
    "    total_rows = len(real_labels)\n",
    "    for i in range(total_rows):\n",
    "        if(real_labels[i] == predicted[i]):\n",
    "            correct += 1\n",
    "    print(\"Correct labels: \", correct, 'of', (total_rows))\n",
    "    accuracy = correct / float(total_rows)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear la función que calcule los vecinos más cercanos.\n",
    "#### Summary:\n",
    "      Se asignan los parámetros para calcular los k-vecinos más cercanos y hacer predicciones\n",
    "      de las etiquetas a las que pertenecen, calculando la distancia entre las columnas de cada\n",
    "      uno de los renglones del dataframe de \"test\" y el de \"train\", comparando las \n",
    "      reales con las otenidas por el clasificador y, finalmente, dado el porcentaje de exactitud obtenido. \n",
    "#### Args: \n",
    "      train(pyspark.rdd.RDD): Recibe el conjunto de entrenamiento\n",
    "      test(pyspark.rdd.RDD): Recibe el conjunto de test\n",
    "      k(int): Número de vecinos que se desean obtener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(train, test, k):\n",
    "    predictions = []\n",
    "    total_test_rows = test.count()\n",
    "    for index in range(total_test_rows):\n",
    "        test_row = np.array(test.zipWithIndex().filter(lambda element: element[1] == index).map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "        output = predict_classification(train, test_row, k)\n",
    "        predictions.append(output)\n",
    "    labels_array = np.array(test.map(lambda x: x[0]).collect(), dtype = float)\n",
    "    mean_accuracy = accuracy(labels_array, predictions)\n",
    "    print(\"Mean accuracy: \" + str(mean_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se cargan los datos al dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .option(\"header\", \"false\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .load(\"../data/url_svmlight/Dimension_500_x_1000.svm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm = data_norm.drop('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=SparseVector(997, {3: 0.0581, 4: 0.0966, 5: 0.1176, 15: 0.1, 16: 0.7756, 17: 0.8039, 18: 0.2069, 22: 0.1429, 119: 0.3333, 125: 0.5, 130: 1.0, 131: 0.5, 132: 1.0, 133: 0.5, 135: 0.5, 137: 0.5, 138: 1.0, 140: 1.0, 142: 1.0, 144: 1.0, 146: 1.0, 147: 0.5, 148: 1.0, 269: 0.1429, 667: 1.0}))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features_norm=SparseVector(997, {3: 0.0039, 4: 0.0065, 5: 0.008, 15: 0.0068, 16: 0.0525, 17: 0.0544, 18: 0.014, 22: 0.0097, 119: 0.0226, 125: 0.0338, 130: 0.0677, 131: 0.0338, 132: 0.0677, 133: 0.0338, 135: 0.0338, 137: 0.0338, 138: 0.0677, 140: 0.0677, 142: 0.0677, 144: 0.0677, 146: 0.0677, 147: 0.0338, 148: 0.0677, 269: 0.0097, 667: 0.0677}))]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividir los datos en conjunto de train y de test\n",
    "seed = 1234\n",
    "splits = data_norm.randomSplit([0.7, 0.3], seed)\n",
    "\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# Se asignan los RDD para el posterior procesamiento\n",
    "rdd_train = train.rdd\n",
    "rdd_test = test.rdd\n",
    "# rdd_total = data.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se llama al método y se envían los parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "k_nearest_neighbors(rdd_train, rdd_test, k = 1)\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time, end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de cada método de K-NN  con el archivo Dimensión 5 x 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se asignan los RDD para el posterior procesamiento\n",
    "rdd_train = train.rdd\n",
    "rdd_test = test.rdd\n",
    "rdd_total = data.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se agrega un índice a las instancias para poder recorrerlas posteriormente mediante un filtro.\n",
    "rdd_index = rdd_total.zipWithIndex()\n",
    "# Se selecciona solo la columna que contiene los valores de las características.\n",
    "rdd_columns = rdd_total.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1[0][75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución:  0.0947568416595459\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Se prueba el método de distancia euclideana con RDD\n",
    "# Renglón no. 1\n",
    "rdd_row1 = rdd_index.filter(lambda x: x[1] == 0)\n",
    "# Se transforma en un array de Numpy solo con los valores de las columnas\n",
    "row1 = np.array(rdd_row1.map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "# Las distancias se almacenan en un RDD\n",
    "rdd_distances = rdd_columns.map(lambda x: euclidean_distance(row1, x))\n",
    "start_time = time.time()\n",
    "rdd_distances.collect()\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time,end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 1.4404646590152195), (0.0, 1.7914667218708056), (0.0, 2.0356611659021397)]\n",
      "Tiempo de ejecución:  0.03345060348510742\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Prueba de la función get_neighbors()\n",
    "# Renglón no. 1\n",
    "rdd_row1 = rdd_index.filter(lambda x: x[1] == 0)\n",
    "# Se transforma en un array de Numpy solo con los valores de las columnas\n",
    "row1 = np.array(rdd_row1.map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "start_time = time.time()\n",
    "print(get_neighbors(rdd_total, row1, 3))\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time,end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected label: 0, Got: 0.\n",
      "Tiempo de ejecución:  0.6830856800079346\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Prueba de la función predict_classification()\n",
    "# Renglón no. 1\n",
    "rdd_row1 = rdd_index.filter(lambda x: x[1] == 0)\n",
    "# Se transforma en un array de Numpy solo con los valores de las columnas\n",
    "row1 = np.array(rdd_row1.map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "start_time = time.time()\n",
    "prediction = predict_classification(rdd_total, row1, 3)\n",
    "print('Expected label: %d, Got: %d.' % (rdd_row1.take(1)[0][0][0], prediction))\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time,end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmento de pruebas locas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#    .master(\"local[6]\") \\\n",
    "#    .appName(\"Data exploration URL - KNN Spark RDD\") \\\n",
    "#    .config(\"spark.executor.memory\", \"4gb\") \\\n",
    "#    .getOrCreate()\n",
    "\n",
    "# sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .option(\"header\", \"false\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .load(\"/home/jsarabia/Documents/IA/Data-exploration-url_svmlight/data/url_svmlight/Dimension_100_x_50000.svm\")\n",
    "# Split the data into train and tes100\n",
    "seed = random.randrange(500, 1300, 2)\n",
    "splits = data.randomSplit([0.7, 0.3], 1234)\n",
    "\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# Se asignan los RDD para el posterior procesamiento\n",
    "rdd_train = train.rdd\n",
    "rdd_test = test.rdd\n",
    "# rdd_total = data.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.ml.feature import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features=SparseVector(49415, {3: 0.0373, 4: 0.0552, 5: 0.1176, 10: 0.2857, 20: 0.2857, 21: 0.0005, 22: 0.001, 27: 1.0, 32: 0.1111, 53: 1.0, 55: 1.0, 61: 1.0, 63: 1.0, 65: 1.0, 67: 1.0, 69: 1.0, 71: 1.0, 73: 1.0, 81: 1.0, 83: 1.0, 85: 1.0, 87: 1.0, 91: 1.0, 93: 1.0, 101: 1.0, 103: 1.0, 105: 1.0, 107: 1.0, 111: 1.0, 154: 1.0, 174: 1.0, 179: 1.0, 184: 1.0, 187: 1.0, 188: 1.0, 189: 1.0, 193: 0.001, 385: 1.0, 487: 1.0, 490: 1.0, 577: 1.0, 581: 1.0, 2853: 1.0, 2854: 1.0, 2855: 1.0, 2856: 1.0}), features_norm=SparseVector(49415, {3: 0.001, 4: 0.0015, 5: 0.0031, 10: 0.0075, 20: 0.0075, 21: 0.0, 22: 0.0, 27: 0.0264, 32: 0.0029, 53: 0.0264, 55: 0.0264, 61: 0.0264, 63: 0.0264, 65: 0.0264, 67: 0.0264, 69: 0.0264, 71: 0.0264, 73: 0.0264, 81: 0.0264, 83: 0.0264, 85: 0.0264, 87: 0.0264, 91: 0.0264, 93: 0.0264, 101: 0.0264, 103: 0.0264, 105: 0.0264, 107: 0.0264, 111: 0.0264, 154: 0.0264, 174: 0.0264, 179: 0.0264, 184: 0.0264, 187: 0.0264, 188: 0.0264, 189: 0.0264, 193: 0.0, 385: 0.0264, 487: 0.0264, 490: 0.0264, 577: 0.0264, 581: 0.0264, 2853: 0.0264, 2854: 0.0264, 2855: 0.0264, 2856: 0.0264}))]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new2 = df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new2 = df_new2.drop('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features_norm=SparseVector(49415, {3: 0.001, 4: 0.0015, 5: 0.0031, 10: 0.0075, 20: 0.0075, 21: 0.0, 22: 0.0, 27: 0.0264, 32: 0.0029, 53: 0.0264, 55: 0.0264, 61: 0.0264, 63: 0.0264, 65: 0.0264, 67: 0.0264, 69: 0.0264, 71: 0.0264, 73: 0.0264, 81: 0.0264, 83: 0.0264, 85: 0.0264, 87: 0.0264, 91: 0.0264, 93: 0.0264, 101: 0.0264, 103: 0.0264, 105: 0.0264, 107: 0.0264, 111: 0.0264, 154: 0.0264, 174: 0.0264, 179: 0.0264, 184: 0.0264, 187: 0.0264, 188: 0.0264, 189: 0.0264, 193: 0.0, 385: 0.0264, 487: 0.0264, 490: 0.0264, 577: 0.0264, 581: 0.0264, 2853: 0.0264, 2854: 0.0264, 2855: 0.0264, 2856: 0.0264}))]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = random.randrange(500, 1300, 2)\n",
    "splits = df_new2.randomSplit([0.7, 0.3], 1234)\n",
    "\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# Se asignan los RDD para el posterior procesamiento\n",
    "rdd_train = train.rdd\n",
    "rdd_test = test.rdd\n",
    "# rdd_total = data.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct labels:  56 of 69\n",
      "Mean accuracy: 0.8115942028985508\n",
      "Tiempo de ejecución:  21.329047699769337\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "k_nearest_neighbors(rdd_train, rdd_test, k = 1)\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time, end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         ss_features|\n",
      "+--------------------+\n",
      "|(987,[1,3,4,5,9,1...|\n",
      "|(987,[3,4,5,10,20...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_data.select('ss_features').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_row1 = rdd_index.filter(lambda x: x[1] == 0)\n",
    "rdd_row2 = rdd_index.filter(lambda x: x[1] == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_array = np.array(rdd_test.map(lambda x: x[0]).collect(), dtype = float)\n",
    "labels_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución:  31.2671480178833\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "euclidean_distance(rdd_row1, rdd_row2)\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time,end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución:  18.672855377197266\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "instancias = rdd_total.count()\n",
    "rdd_row1 = rdd_index.filter(lambda x: x[1] == 0)\n",
    "row1 = np.array(rdd_row1.map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "for i in range(instancias):\n",
    "    rdd_row2 = rdd_index.filter(lambda x: x[1] == i)\n",
    "    row2 = np.array(rdd_row2.map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "    distance = euclidean_distance(rdd_row1, rdd_row2)\n",
    "    # print(\"Dinstancia del renglon 1 con el\", i, \":\", distance) \n",
    "end_time = time.time()\n",
    "print(tiempo(start_time,end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "length = len(total_array)\n",
    "for row in range(length):\n",
    "    distance = euclidean_distance(total_array[0], total_array[row])\n",
    "    #print(\"Dinstancia del renglon 1 con el\", row, \":\", distance)    \n",
    "end_time = time.time()\n",
    "print(tiempo(start_time,end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se utilizan dos array de numpy para alamacenar las instancias del set de entrenamiento y procesar con un RDD y el segundo array almacena las etiquetas. \n",
    "#train_array = np.array(train.select('features').collect(), dtype=float)\n",
    "#train_array_labels = np.array(train.select('label').collect(), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiquteas de las instancias del conjunto de test. \n",
    "train_array = np.array(train.collect(), dtype=object)\n",
    "test_array = np.array(test.collect(), dtype=object)\n",
    "total_array = np.array(data.collect(), dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0,\n",
       "       SparseVector(494785, {3: 0.0373, 4: 0.0552, 5: 0.1176, 10: 0.2857, 20: 0.2857, 21: 0.0005, 22: 0.001, 27: 1.0, 32: 0.1111, 53: 1.0, 55: 1.0, 61: 1.0, 63: 1.0, 65: 1.0, 67: 1.0, 69: 1.0, 71: 1.0, 73: 1.0, 81: 1.0, 83: 1.0, 85: 1.0, 87: 1.0, 91: 1.0, 93: 1.0, 101: 1.0, 103: 1.0, 105: 1.0, 107: 1.0, 111: 1.0, 154: 1.0, 174: 1.0, 179: 1.0, 184: 1.0, 187: 1.0, 188: 1.0, 189: 1.0, 193: 0.001, 385: 1.0, 487: 1.0, 490: 1.0, 577: 1.0, 581: 1.0, 2853: 1.0, 2854: 1.0, 2855: 1.0, 2856: 1.0, 51675: 1.0, 51676: 1.0, 51677: 1.0, 51683: 1.0, 64598: 1.0, 106458: 1.0, 155152: 1.0, 155153: 1.0, 155154: 1.0, 155155: 1.0, 155156: 1.0, 155161: 1.0, 155162: 1.0, 155163: 1.0, 155164: 1.0, 155165: 1.0, 155167: 1.0, 155168: 1.0, 155169: 1.0, 155171: 1.0, 155172: 1.0, 155173: 1.0, 155174: 1.0, 155175: 1.0, 155176: 1.0, 155177: 1.0, 155178: 1.0, 155179: 1.0, 155180: 1.0, 155181: 1.0, 155182: 1.0, 155193: 1.0, 155194: 1.0, 155195: 1.0, 155196: 1.0, 155197: 1.0, 155198: 1.0, 155199: 1.0, 155200: 1.0, 155201: 1.0, 155202: 1.0, 155203: 1.0, 155204: 1.0, 155205: 1.0, 155206: 1.0, 155207: 1.0, 155208: 1.0, 155209: 1.0, 155210: 1.0, 155211: 1.0, 155212: 1.0})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución:  0.18266701698303223\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "length = len(total_array)\n",
    "for row in range(length):\n",
    "    distance = euclidean_distance(total_array[0], total_array[row])\n",
    "    #print(\"Dinstancia del renglon 1 con el\", row, \":\", distance)    \n",
    "end_time = time.time()\n",
    "print(tiempo(start_time,end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0\n",
      " SparseVector(100, {3: 0.1037, 4: 0.1655, 5: 0.1176, 10: 0.2857, 20: 0.0238, 21: 0.0238, 27: 1.0, 43: 1.0, 53: 1.0, 55: 1.0, 61: 1.0, 63: 1.0, 65: 1.0, 67: 1.0, 69: 1.0, 71: 1.0, 73: 1.0, 75: 1.0, 81: 1.0, 83: 1.0, 85: 1.0, 87: 1.0, 89: 1.0, 91: 1.0, 93: 1.0, 95: 1.0})]\n",
      "[0.0\n",
      " SparseVector(100, {3: 0.083, 4: 0.131, 5: 0.1176, 10: 0.2857, 20: 0.006, 21: 0.006, 27: 1.0, 32: 0.0556, 40: 0.1, 43: 1.0, 53: 1.0, 55: 1.0, 61: 1.0, 63: 1.0, 65: 1.0, 67: 1.0, 69: 1.0, 71: 1.0, 73: 1.0, 75: 1.0, 81: 1.0, 83: 1.0, 85: 1.0, 87: 1.0, 89: 1.0, 91: 1.0, 93: 1.0, 95: 1.0})]\n",
      "[0.0\n",
      " SparseVector(100, {3: 0.0913, 4: 0.1448, 5: 0.1765, 10: 0.1429, 20: 0.006, 21: 0.006, 23: 1.0, 24: 1.0, 27: 1.0, 32: 0.0556, 40: 0.1, 53: 1.0, 61: 1.0, 63: 1.0, 65: 1.0, 67: 1.0, 71: 1.0, 73: 1.0, 80: 0.05, 81: 1.0, 83: 1.0, 85: 1.0, 87: 1.0, 91: 1.0, 93: 1.0})]\n",
      "Tiempo de ejecución:  0.2047569751739502\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Prueba de la función get_neighbors(), se envían como args. el dataframe, un renglón de este y el número de vecinos.\n",
    "start_time = time.time()\n",
    "length = data.count() + 1\n",
    "neighbors = get_neighbors(total_array, total_array[0], k=3)\n",
    "for neighbor in neighbors:\n",
    "    print(neighbor)\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time, end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row class: 0.0\n",
      "[0.0, 0.0, 0.0]\n",
      "0.0\n",
      "==================\n",
      "Expected label: 0, Got: 0.\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "print(\"Row class:\", data.head(n)[-1][0] ) # CLase/Label\n",
    "prediction = predict_classification(total_array, total_array[0], k=3)\n",
    "print('Expected label: %d, Got: %d.' % (total_array[0][0], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(test_array[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "k_nearest_neighbors(train_array, test_array, k = 3)\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time, end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD de entrenamiento: 131\n",
      "RDD de test: 69\n"
     ]
    }
   ],
   "source": [
    "print('RDD de entrenamiento: ' + str(rdd_train.count()))\n",
    "print('RDD de test: ' + str(rdd_test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo que guarda cada renglon en un archivo .svm\n",
    "def save_file(data):\n",
    "    file = open('../data/url_svmlight/Distancia_euclideana_100_x_500000.svm', 'a')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[summary:\n",
    "    Método para calcular la distancia euclídea entre cada una de las \n",
    "    columnas del conjunto de test respecto a las columnas del conjunto\n",
    "    de entrenamiento.\n",
    "]\n",
    "\n",
    "Args:\n",
    "    instance ([pyspark.sql.types.Row]): [\n",
    "        Recibe cada una de las instancias que hay en el dataset\n",
    "    ]\n",
    "\"\"\"\n",
    "def euclidean_distance(instance):\n",
    "    distance = 0\n",
    "    instance_distance = ''\n",
    "    for row in range(len(train_array)):\n",
    "        instance_distance += str(train_array_labels[row][0]) + ' '\n",
    "        for column in range(len(instance[1])):\n",
    "            distance = pow(train_array[row][0][column] - instance.features[column], 2)\n",
    "            distance = math.sqrt(distance)\n",
    "            # instance_distance += str(column + 1) +':' + str(distance) + ' ' # -> Si quisiera poner los indices de cada caracteristica.\n",
    "            instance_distance += str(distance) + ' '\n",
    "        instance_distance += '\\n'\n",
    "    save_file(instance_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecuta el método que calcula la distancia euclídea entre los puntos euclidean_distance()\n",
    "test.foreach(euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_samp1 = sc.textFile('../data/url_svmlight/arch_prb.svm')\n",
    "rdd_samp2 = sc.textFile('../data/url_svmlight/arch_prb1.svm')\n",
    "rdd_samp3 = sc.textFile('../data/url_svmlight/arch_prb2.svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_nearest1 = rdd_samp1.takeOrdered(5)\n",
    "five_nearest2 = rdd_samp2.takeOrdered(5)\n",
    "five_nearest3 = rdd_samp3.takeOrdered(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_average(five_nearest):\n",
    "    mean = 0\n",
    "    for i in range(5):\n",
    "        mean += float(five_nearest[i][0])\n",
    "    mean = mean / 5\n",
    "    if(mean > 0.5):\n",
    "        print('Clase K-NN: 1')\n",
    "        return 1\n",
    "    else:\n",
    "        print('Clase K-NN: 0')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy():\n",
    "    lista = [five_nearest1, five_nearest2, five_nearest3]\n",
    "    accuracy = 0.0\n",
    "    for i in range(len(test_array_labels)):\n",
    "        if(test_array_labels[i][0] == class_average(lista[i])):\n",
    "            accuracy += 1\n",
    "        print('Clase Real: ' + str(test_array_labels[i][0]))\n",
    "        print('\\n')\n",
    "    accuracy = accuracy / len(test_array_labels)\n",
    "    print('Accuracy: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = [five_nearest1, five_nearest2, five_nearest3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listaclass_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# five_nearest[0][0] # Clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_prueba2 = sc.textFile('../data/url_svmlight/Distancia_euclideana_5_x_76.svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_nearest2 = rdd_prueba.takeOrdered(5)\n",
    "type(five_nearest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_nearest2[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución:  9.42785382270813\n"
     ]
    }
   ],
   "source": [
    "# Prueba de la función euclidean_distance(), se mandan dos renglones del dataset total\n",
    "\n",
    "start  = time.time()\n",
    "length = data.count()               # Se obtiene el total de renglones en el dataset\n",
    "for row in range(1, (length + 1)):\n",
    "    distance = euclidean_distance(data.head(1)[-1], data.head(row)[-1])\n",
    "    #print(\"Dinstancia del renglon 1 con el\", row, \":\", distance)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "tiempo(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución:  0.03139948844909668\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "data.head(175)[-1]\n",
    "end = time.time()\n",
    "\n",
    "tiempo(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`* where label=0`' given input columns: [features, label];\n'Project ['* where label=0]\n+- Relation[label#10,features#11] libsvm\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c03f866c4fd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'* where label=0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \"\"\"\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`* where label=0`' given input columns: [features, label];\n'Project ['* where label=0]\n+- Relation[label#10,features#11] libsvm\n"
     ]
    }
   ],
   "source": [
    "data.select('* where label=0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)[-1][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0456432\n",
      "0.0758621\n",
      "0.0588235\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.142857\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.1\n",
      "0.895527\n",
      "0.839999\n",
      "0.129503\n",
      "0.0\n",
      "0.00595074\n",
      "0.00595238\n",
      "0.00595238\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0555556\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(76):\n",
    "    print(data.head(10)[-1][1][i])\n",
    "    #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd04b885024a669b25ee4a71b7d0638ae4cd28c0f16f4c7c66f708405d8a6800548"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "4b885024a669b25ee4a71b7d0638ae4cd28c0f16f4c7c66f708405d8a6800548"
   }
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
