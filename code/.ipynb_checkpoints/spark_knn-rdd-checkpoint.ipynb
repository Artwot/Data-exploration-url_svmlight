{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador K-NN en Spark usando pyspark.RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se importan las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np \n",
    "import sys \n",
    "import os\n",
    "\n",
    "#path = str(sys.argv[1])\n",
    "#atr = int(sys.argv[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se crea la sesión y config. de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf()\n",
    "        .setAppName(\"Data exploration URL - KNN Spark RDD\") \\\n",
    "        .set('spark.driver.cores', '6') \\\n",
    "        .set('spark.executor.cores', '6') \\\n",
    "        .set('spark.driver.memory', '6G') \\\n",
    "        .set('spark.master', 'local[6]') \\\n",
    "        .set('spark.sql.autoBroadcastJoinThreshold', '-1') \\\n",
    "        .set('spark.executor.memory', '6G'))\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', '192.168.0.9'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.memory', '6G'),\n",
       " ('spark.master', 'local[6]'),\n",
       " ('spark.driver.cores', '6'),\n",
       " ('spark.executor.memory', '6G'),\n",
       " ('spark.executor.cores', '6'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1620664024126'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '42325'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.autoBroadcastJoinThreshold', '-1'),\n",
       " ('spark.app.startTime', '1620664011312'),\n",
       " ('spark.app.name', 'Data exploration URL - KNN Spark RDD'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.9:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[6]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Data exploration URL - KNN Spark RDD</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[6] appName=Data exploration URL - KNN Spark RDD>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para calcular el tiempo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiempo(start, end):\n",
    "    medida = 'segundos'\n",
    "    tiempo = end - start\n",
    "    if (tiempo >= 60):\n",
    "        tiempo = tiempo / 60\n",
    "        medida = 'minutos'\n",
    "    else:\n",
    "        if (tiempo >= 3600):\n",
    "            tiempo = tiempo / 3600\n",
    "            medida = 'horas'\n",
    "    print(\"Tiempo de ejecución: \", round(tiempo, 2), medida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular la distancia euclideana.\n",
    "#### Summary:\n",
    "        Se calcula la distancia entre las columnas de dos renglones de un dataset, funciona\n",
    "        con argumentos provenientes de un renglón de un dataframe de Spark.\n",
    "#### Args: \n",
    "        row1(numpy.ndarray): Recibe una instancia del dataset\n",
    "        row2(pyspark.ml.linalg.SparseVector): Recibe una instancia del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    columns = len(row1[0])\n",
    "    for column in range(columns):\n",
    "        distance += pow(row1[0][column] - row2[column], 2)\n",
    "    distance = math.sqrt(distance)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener los vecinos más cercanos.\n",
    "#### Summary: \n",
    "      Se recorre cada renglón del dataframe dado y se calcula la distancia entre cada \n",
    "      uno de estos y el renglón de prueba.\n",
    "      El RDD \"distances\", almacenará las distancias calculadas, \n",
    "      posteriormente se ordena de modo ascendente y se almancenan los primeros k-elementos \n",
    "      en la lista \"k_neighbors\"\n",
    "\n",
    "#### Args: \n",
    "      train(pyspark.rdd.RDD): Recibe el conjunto de entrenamiento\n",
    "      test_row(numpy.ndarray): Recibe una instancia del conjunto de test\n",
    "      k(int): Número de vecinos que se desean obtener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(train, test_row, k):\n",
    "    rdd_distances = train.map(lambda element: (element[0], euclidean_distance(test_row, element[1])))\n",
    "    rdd_distances = rdd_distances.filter(lambda element: element[1] > 0.0)\n",
    "    rdd_distances.repartition(50)\n",
    "    k_neighbors = rdd_distances.takeOrdered(k, key= lambda  x: x[1]) \n",
    "    return k_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predecir las etiquetas usando k-nn.\n",
    "#### Summary:\n",
    "      Se obtiene la lista de los k-vecinos más cercanos, y se almacena el valor de\n",
    "      la etiqueta en la lista \"output_labels\". Posteriormente se calcula el valor \n",
    "      promedio de las etiquetas y se almacena en la variable \"prediction\" y se retorna.\n",
    "\n",
    "#### Args: \n",
    "      train(pyspark.rdd.RDD): Recibe el conjunto de entrenamiento\n",
    "      test_row(numpy.ndarray): Recibe una instancia del conjunto de test\n",
    "      k(int): Número de vecinos que se desean obtener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classification(train, test_row, k):\n",
    "    neighbors = get_neighbors(train, test_row, k)\n",
    "    output_labels = [row[0] for row in neighbors]\n",
    "    prediction = max(set(output_labels), key=output_labels.count)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clacular el porcentaje de exactitud.\n",
    "#### Summary:\n",
    "      Esta función calcula el porcentaje de exactitud del uso de k-NN, comparando\n",
    "      las etiquetas reales de las instancias del dataset de entrenamiento y las\n",
    "      etiquetas obtenidas mediante la predicción usando k-NN.\n",
    "#### Args: \n",
    "      real_labels(numpy.ndarray): Recibe el dataframe de test que contiene los\n",
    "                                                    valores reales de las etiquetas\n",
    "      predicted(list): Lista con las etiquetas obtenidas mediante K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(real_labels, predicted):\n",
    "    correct = 0\n",
    "    total_rows = len(real_labels)\n",
    "    for i in range(total_rows):\n",
    "        if(real_labels[i] == predicted[i]):\n",
    "            correct += 1\n",
    "    print(\"Correct labels: \", correct, 'of', (total_rows))\n",
    "    accuracy = correct / float(total_rows)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear la función que calcule los vecinos más cercanos.\n",
    "#### Summary:\n",
    "      Se asignan los parámetros para calcular los k-vecinos más cercanos y hacer predicciones\n",
    "      de las etiquetas a las que pertenecen, calculando la distancia entre las columnas de cada\n",
    "      uno de los renglones del dataframe de \"test\" y el de \"train\", comparando las \n",
    "      reales con las otenidas por el clasificador y, finalmente, dado el porcentaje de exactitud obtenido. \n",
    "#### Args: \n",
    "      train(pyspark.rdd.RDD): Recibe el conjunto de entrenamiento\n",
    "      test(pyspark.rdd.RDD): Recibe el conjunto de test\n",
    "      k(int): Número de vecinos que se desean obtener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(train, test, k):\n",
    "    predictions = []\n",
    "    total_test_rows = test.count()\n",
    "    for index in range(total_test_rows):\n",
    "        test_row = np.array(test.zipWithIndex().filter(lambda element: element[1] == index).map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "        output = predict_classification(train, test_row, k)\n",
    "        predictions.append(output)\n",
    "    labels_array = np.array(test.map(lambda x: x[0]).collect(), dtype = float)\n",
    "    mean_accuracy = accuracy(labels_array, predictions)\n",
    "    print(\"Mean accuracy: \" + str(mean_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se cargan los datos al dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .option(\"header\", \"false\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .load(\"../data/url_svmlight/1615981_x_instances_30.svm\")\n",
    "    # .load(\"/home/jsarabia/Documents/IA/datasets/Data-exploration/807990_x_instances_30.svm\")\n",
    "    # .load(\"../data/url_svmlight/Dimension_100_x_1000.svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"features_norm\")\n",
    "\n",
    "# Compute summary statistics and generate MaxAbsScalerModel\n",
    "scalerModel = scaler.fit(data)\n",
    "\n",
    "# rescale each feature to range [-1, 1].\n",
    "scaledData = scalerModel.transform(data)\n",
    "\n",
    "scaledData = scaledData.drop(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features_norm=SparseVector(1585224, {1: 1.0, 3: 0.3167, 4: 0.6333, 5: 0.5, 9: 1.0, 10: 0.1667, 16: 0.7213, 17: 0.9984, 18: 0.2645, 20: 0.0197, 21: 1.0, 23: 1.0, 27: 1.0, 35: 1.0, 43: 1.0, 53: 1.0, 55: 1.0, 61: 1.0, 63: 1.0, 65: 1.0, 67: 1.0, 69: 1.0, 71: 1.0, 73: 1.0, 75: 1.0, 81: 1.0, 83: 1.0, 85: 1.0, 87: 1.0, 89: 1.0, 91: 1.0, 93: 1.0, 95: 1.0, 101: 1.0, 103: 1.0, 105: 1.0, 107: 1.0, 109: 1.0, 111: 1.0, 154: 1.0, 409: 1.0, 414: 1.0, 497: 1.0, 1545: 1.0, 1546: 1.0, 1547: 1.0, 1549: 1.0, 26007: 1.0, 26008: 1.0, 26009: 1.0, 28181: 1.0, 28182: 1.0, 28183: 1.0, 155152: 1.0, 155153: 1.0, 155154: 1.0, 155155: 1.0, 155156: 1.0, 155157: 1.0, 155158: 1.0, 155159: 1.0, 155162: 1.0, 155163: 1.0, 155165: 1.0, 155167: 1.0, 155168: 1.0, 155169: 1.0, 155171: 1.0, 155172: 1.0, 155173: 1.0, 155174: 1.0, 155175: 1.0, 155176: 1.0, 155177: 1.0, 155178: 1.0, 155179: 1.0, 155180: 1.0, 155181: 1.0, 155182: 1.0, 155193: 1.0, 155194: 1.0, 155195: 1.0, 155196: 1.0, 155197: 1.0, 155198: 1.0, 155199: 1.0, 155200: 1.0, 155201: 1.0, 155202: 1.0, 155203: 1.0, 155204: 1.0, 155205: 1.0, 155206: 1.0, 155207: 1.0, 155208: 1.0, 155209: 1.0, 155210: 1.0, 155211: 1.0, 155212: 1.0, 422764: 1.0, 1524911: 1.0}))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dividir los datos en conjunto de train y de test\n",
    "seed = 1234\n",
    "splits = scaledData.randomSplit([0.7, 0.3], seed)\n",
    "\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# Se asignan los RDD para el posterior procesamiento\n",
    "rdd_train = train.rdd\n",
    "rdd_test = test.rdd\n",
    "# rdd_total = data.rdd\n",
    "\n",
    "scaledData.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_test.getNumPartitions()\n",
    "rdd_train1 = rdd_train.repartition(50)\n",
    "rdd_test1 = rdd_test.repartition(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_train1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_test1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se invoca al método y se envían los parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct labels:  12 of 18\n",
      "Mean accuracy: 0.6666666666666666\n",
      "Tiempo de ejecución:  71.68 minutos\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "k_nearest_neighbors(rdd_train1, rdd_test1, k = 7)\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time, end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark session stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de cada método de K-NN  con el archivo Dimensión 5 x 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se asignan los RDD para el posterior procesamiento\n",
    "rdd_train = train.rdd\n",
    "rdd_test = test.rdd\n",
    "rdd_total = data.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se agrega un índice a las instancias para poder recorrerlas posteriormente mediante un filtro.\n",
    "rdd_index = rdd_total.zipWithIndex()\n",
    "# Se selecciona solo la columna que contiene los valores de las características.\n",
    "rdd_columns = rdd_total.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución:  0.08 segundos\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Se prueba el método de distancia euclideana con RDD\n",
    "# Renglón no. 1\n",
    "rdd_row1 = rdd_index.filter(lambda x: x[1] == 0)\n",
    "# Se transforma en un array de Numpy solo con los valores de las columnas\n",
    "row1 = np.array(rdd_row1.map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "# Las distancias se almacenan en un RDD\n",
    "rdd_distances = rdd_columns.map(lambda x: euclidean_distance(row1, x))\n",
    "start_time = time.time()\n",
    "rdd_distances.collect()\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time,end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "0\n",
      "Tiempo de ejecución:  0.69 segundos\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Prueba de la función get_neighbors()\n",
    "# Renglón no. 1\n",
    "rdd_row1 = rdd_index.filter(lambda x: x[1] == 0)\n",
    "# Se transforma en un array de Numpy solo con los valores de las columnas\n",
    "row1 = np.array(rdd_row1.map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "start_time = time.time()\n",
    "print(get_neighbors(rdd_total, row1, k = 5))\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time,end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected label: 0, Got: 0.\n",
      "Tiempo de ejecución:  0.6830856800079346\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\t20 s# Prueba de la función predict_classification()\n",
    "# Renglón no. 1\n",
    "rdd_row1 = rdd_index.filter(lambda x: x[1] == 0)\n",
    "# Se transforma en un array de Numpy solo con los valores de las columnas\n",
    "row1 = np.array(rdd_row1.map(lambda element: element[0][1]).collect(), dtype = object)\n",
    "start_time = time.time()\n",
    "prediction = predict_classification(rdd_total, row1, 3)\n",
    "print('Expected label: %d, Got: %d.' % (rdd_row1.take(1)[0][0][0], prediction))\n",
    "end_time = time.time()\n",
    "print(tiempo(start_time,end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python385jvsc74a57bd04b885024a669b25ee4a71b7d0638ae4cd28c0f16f4c7c66f708405d8a6800548"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "4b885024a669b25ee4a71b7d0638ae4cd28c0f16f4c7c66f708405d8a6800548"
   }
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
